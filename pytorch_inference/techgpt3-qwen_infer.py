import json
import torch
import uvicorn
import threading
import time
from fastapi import FastAPI, Request
from transformers import AutoModelForCausalLM, AutoTokenizer
import requests

app = FastAPI()

# âœ… æ¨¡å‹è·¯å¾„ï¼ˆè¯·æ›¿æ¢ä¸ºä½ è‡ªå·±çš„æ¨¡å‹è·¯å¾„ï¼‰
model_name = "your model path"
device = "cuda"

print("ğŸ”„ åŠ è½½æ¨¡å‹ä¸­...")
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)
print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

@app.post("/question_answer")
async def create_item(request: Request):
    json_post_raw = await request.json()
    json_post_list = json.loads(json.dumps(json_post_raw))

    prompt = json_post_list.get('prompt')
    history = json_post_list.get('history') or []
    if prompt is None:
        return {"response": "Promptä¸èƒ½ä¸ºç©º", "history": []}

    system_prompt = [{"role": "system", "content": "You are a helpful assistant."}]
    current_prompt = [{"role": "user", "content": prompt}]
    messages = system_prompt + history + current_prompt
    messages = [m for m in messages if m.get("content") is not None]

    try:
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=True,
        )
    except Exception as e:
        return {"response": f"Chatæ¨¡æ¿æ¸²æŸ“é”™è¯¯: {str(e)}", "history": history}

    model_inputs = tokenizer([text], return_tensors="pt").to(device)

    # âœ… ä½¿ç”¨æ€è€ƒæ¨¡å¼æ¨èå‚æ•°è¿›è¡Œç”Ÿæˆ
    generated_ids = model.generate(
        **model_inputs,
        temperature=0.6,
        top_p=0.95,
        top_k=20,
        min_p=0,
        max_new_tokens=4096,
        do_sample=True  # ç¦æ­¢è´ªå©ªè§£ç 
    )
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]

    response_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

    history = history + current_prompt
    history.append({"role": "assistant", "content": response_text})
    print("ğŸ“¤ Chat response:", response_text)
    return {"response": response_text, "history": history}

def call_api():
    time.sleep(5)  # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
    print("ğŸš€ å¼€å§‹æµ‹è¯•è°ƒç”¨ API...")

    # âœ… æµ‹è¯•ç”¨ API åœ°å€ï¼ˆè¯·æ›¿æ¢ä¸ºå®é™…ç«¯å£ï¼‰
    url = "http://localhost:your_port/question_answer"
    payload = {
        "prompt": "å¸‚æ”¿åºœå†³å®šä»2025å¹´7æœˆèµ·ï¼Œå…¨é¢æ¨è¡Œåƒåœ¾åˆ†ç±»åˆ¶åº¦ã€‚è¯·å°†ä¸Šè¿°é€šå‘Šæ‰©å±•ä¸ºä¸€åˆ™æ­£å¼å®Œæ•´çš„æ–°é—»é€šç¨¿ï¼Œå†…å®¹åŒ…æ‹¬èƒŒæ™¯ã€æªæ–½ä¸æ„ä¹‰ã€‚",
        "history": []
    }

    response = requests.post(url, json=payload)
    if response.status_code == 200:
        result = response.json()
        print("âœ… æ¨¡å‹å›ç­”:", result["response"])
    else:
        print("âŒ è°ƒç”¨å¤±è´¥:", response.text)

if __name__ == '__main__':
    # âœ… å¯åŠ¨æœåŠ¡å™¨çº¿ç¨‹ï¼ˆè¯·æ›¿æ¢ä¸ºä½ çš„ç«¯å£å·ï¼‰
    server_thread = threading.Thread(
        target=lambda: uvicorn.run(app, host="0.0.0.0", port=your_port, log_level="error"),
        daemon=True
    )
    server_thread.start()

    # å¯åŠ¨å®¢æˆ·ç«¯è°ƒç”¨
    call_api()
